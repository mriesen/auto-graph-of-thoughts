{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Simple PPO Approach\n",
    "As a first approach with Deep Reinforcement Learning (DRL), a simple PPO agent is trained on lists of fixed cardinality.\n",
    "The goal of this first DRL approach is to verify that a complex Reinforcement Learning agent is able to learn a task for a given cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8769a32849ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of cardinality $16$.\n",
    "The complexity equals the list cardinality.\n"
   ],
   "id": "c3227651ebe35466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:21.897754Z",
     "start_time": "2024-05-09T12:20:21.108586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env import GraphObservationComponent, GraphStepRewardVersion\n",
    "from auto_graph_of_thoughts.experiment import ExperimentConfiguration, LanguageModelSimulationType, Experiment\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "ENABLE_TRAINING = True\n",
    "ENABLE_EVALUATION = True\n",
    "\n",
    "COMPLEXITIES = [16]\n",
    "\n",
    "EVAL_N_EPISODES = 100\n",
    "\n",
    "config = ExperimentConfiguration(\n",
    "        seed=SEED,\n",
    "        task=sum_list_task,\n",
    "        max_steps=20,\n",
    "        observation_filter={\n",
    "            GraphObservationComponent.depth,\n",
    "            GraphObservationComponent.breadth,\n",
    "            GraphObservationComponent.complexity,\n",
    "            GraphObservationComponent.prev_actions,\n",
    "            GraphObservationComponent.graph_operations,\n",
    "            GraphObservationComponent.local_complexity,\n",
    "            GraphObservationComponent.prev_score\n",
    "        },\n",
    "        max_depth=8,\n",
    "        max_breadth=8,\n",
    "        divergence_cutoff_factor=0.5,\n",
    "        train_complexities=COMPLEXITIES,\n",
    "        eval_complexities=COMPLEXITIES,\n",
    "        max_complexity=max(COMPLEXITIES),\n",
    "        max_operations=32,\n",
    "        lm_simulation_type=LanguageModelSimulationType.REALISTIC,\n",
    "        reward_version=GraphStepRewardVersion.V4\n",
    ")\n",
    "experiment = Experiment(config)"
   ],
   "id": "59f8c0dfc11c7622",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Reproducibility\n",
    "The seed for the PRNG is set to $0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7c93d09f7f257b"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "seed = 0\n",
    "set_random_seed(seed)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.613461Z",
     "start_time": "2024-05-09T12:20:21.897754Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities",
   "id": "d842a88d911d4546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.621480Z",
     "start_time": "2024-05-09T12:20:24.613461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pure_graph_of_thoughts.api.schema import JsonSchemaEncoder\n",
    "import json\n",
    "import os\n",
    "from auto_graph_of_thoughts.experiment.agent_evaluation_summary import AgentEvaluationSummary\n",
    "\n",
    "results_directory = './artifacts/results/agent_evaluations'\n",
    "\n",
    "def store_evaluation_summary(evaluation_summary: AgentEvaluationSummary) -> None:\n",
    "    \"\"\"\n",
    "    Stores an evaluation summary to file.\n",
    "    :param evaluation_summary: evaluation summary to store\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{evaluation_summary.name}.json'\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_summary, f, cls=JsonSchemaEncoder, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_evaluation_summary(name: str) -> AgentEvaluationSummary:\n",
    "    \"\"\"\n",
    "    Loads an evaluation summary.\n",
    "    :param name: name\n",
    "    :return: loaded evaluation summary\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{name}.json'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return AgentEvaluationSummary.from_dict(json.load(f))\n"
   ],
   "id": "4ba3b6ea0235cd5e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train PPO Model\n",
    "The default PPO model is trained with a vectorized environment (number of environments: `8`).\n",
    "The number of total time steps is set to $2^{18}$ ($262'144$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d786b44a8df86031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.630577Z",
     "start_time": "2024-05-09T12:20:24.621480Z"
    }
   },
   "cell_type": "code",
   "source": "model_ppo_name = 'ppo_r4_64x64_c16_t2xx18_lrfix'",
   "id": "434a25c3f7eb010",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "if ENABLE_TRAINING:\n",
    "    vec_env = make_vec_env(lambda: experiment.create_filtered_train_env(), n_envs=8)\n",
    "    model_ppo = PPO('MultiInputPolicy', vec_env, verbose=1, tensorboard_log='./artifacts/tensorboard')\n",
    "    model_ppo.learn(total_timesteps=2 ** 18, tb_log_name=model_ppo_name)\n",
    "    mean_reward, std_reward = evaluate_policy(model_ppo, model_ppo.get_env(), n_eval_episodes=EVAL_N_EPISODES)\n",
    "    print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "    model_ppo.save(f'./artifacts/models/{model_ppo_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:49.416120Z",
     "start_time": "2024-05-09T12:20:24.630577Z"
    }
   },
   "id": "9ec883333cb3abb1",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate PPO Model\n",
    "The trained PPO model is evaluated on $100$ time steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c56f2a1770a1d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Episodes for PPO",
   "id": "3bb06192170b24e"
  },
  {
   "cell_type": "code",
   "source": [
    "unwrapped_env, filtered_env = experiment.created_eval_env_tuple()\n",
    "model_ppo = PPO.load(f'./artifacts/models/{model_ppo_name}')\n",
    "obs, info = filtered_env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model_ppo.predict(obs)\n",
    "    decoded_action = filtered_env.decode_action(action)\n",
    "    obs, rewards, terminated, truncated, info = filtered_env.step(action)\n",
    "    print(\n",
    "        f'action: {decoded_action.type.name}-{decoded_action.operation.name if decoded_action.operation is not None else None} = {float(rewards)}')\n",
    "    if terminated or truncated:\n",
    "        obs, info = filtered_env.reset()\n",
    "        print(f'Episode end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:49.955884Z",
     "start_time": "2024-05-09T12:30:49.416120Z"
    }
   },
   "id": "37aa87ac5cc986",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Agent Evaluation for PPO",
   "id": "1561835449c805e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:52.967431Z",
     "start_time": "2024-05-09T12:30:49.955884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.experiment.evaluate_agent import evaluate_agent\n",
    "\n",
    "if ENABLE_EVALUATION:\n",
    "    evaluation_ppo = evaluate_agent(\n",
    "            experiment,\n",
    "            model_ppo_name,\n",
    "            EVAL_N_EPISODES,\n",
    "            lambda obs: model_ppo.predict(obs)[0]\n",
    "    )\n",
    "    store_evaluation_summary(evaluation_ppo.summary)\n",
    "    \n",
    "evaluation_ppo_summary = load_evaluation_summary(model_ppo_name)\n",
    "evaluation_ppo_summary.solved_rate_train_complexities, evaluation_ppo_summary.solved_rate_eval_complexities"
   ],
   "id": "2e082c5b5436d544",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:52.974544Z",
     "start_time": "2024-05-09T12:30:52.967431Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation_ppo_summary.avg_n_operations_per_complexity",
   "id": "e8f5446e014fb36a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent is able to solve the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ffa0a5040885f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
