{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Simple PPO Approach\n",
    "As a first approach with Deep Reinforcement Learning (DRL), a simple PPO agent is trained on lists of fixed cardinality.\n",
    "The goal of this first DRL approach is to verify that a complex Reinforcement Learning agent is able to learn a task for a given cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8769a32849ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of cardinality $16$.\n",
    "The complexity equals the list cardinality.\n"
   ],
   "id": "c3227651ebe35466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:21.897754Z",
     "start_time": "2024-05-09T12:20:21.108586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env import GraphObservationComponent, GraphStepRewardVersion\n",
    "from auto_graph_of_thoughts.experiment import ExperimentConfiguration, LanguageModelSimulationType, Experiment\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "ENABLE_TRAINING = True\n",
    "ENABLE_EVALUATION = True\n",
    "\n",
    "COMPLEXITIES = [16]\n",
    "\n",
    "EVAL_N_EPISODES = 100\n",
    "\n",
    "config = ExperimentConfiguration(\n",
    "        seed=SEED,\n",
    "        task=sum_list_task,\n",
    "        max_steps=20,\n",
    "        observation_filter={\n",
    "            GraphObservationComponent.depth,\n",
    "            GraphObservationComponent.breadth,\n",
    "            GraphObservationComponent.complexity,\n",
    "            GraphObservationComponent.prev_actions,\n",
    "            GraphObservationComponent.graph_operations,\n",
    "            GraphObservationComponent.local_complexity,\n",
    "            GraphObservationComponent.prev_score\n",
    "        },\n",
    "        max_depth=8,\n",
    "        max_breadth=8,\n",
    "        divergence_cutoff_factor=0.5,\n",
    "        train_complexities=COMPLEXITIES,\n",
    "        eval_complexities=COMPLEXITIES,\n",
    "        max_complexity=max(COMPLEXITIES),\n",
    "        max_operations=32,\n",
    "        lm_simulation_type=LanguageModelSimulationType.REALISTIC,\n",
    "        reward_version=GraphStepRewardVersion.V4\n",
    ")\n",
    "experiment = Experiment(config)"
   ],
   "id": "59f8c0dfc11c7622",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Reproducibility\n",
    "The seed for the PRNG is set to $0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7c93d09f7f257b"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "seed = 0\n",
    "set_random_seed(seed)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.613461Z",
     "start_time": "2024-05-09T12:20:21.897754Z"
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities",
   "id": "d842a88d911d4546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.621480Z",
     "start_time": "2024-05-09T12:20:24.613461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pure_graph_of_thoughts.api.schema import JsonSchemaEncoder\n",
    "import json\n",
    "import os\n",
    "from auto_graph_of_thoughts.experiment.agent_evaluation_summary import AgentEvaluationSummary\n",
    "\n",
    "results_directory = './artifacts/results/agent_evaluations'\n",
    "\n",
    "def store_evaluation_summary(evaluation_summary: AgentEvaluationSummary) -> None:\n",
    "    \"\"\"\n",
    "    Stores an evaluation summary to file.\n",
    "    :param evaluation_summary: evaluation summary to store\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{evaluation_summary.name}.json'\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_summary, f, cls=JsonSchemaEncoder, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_evaluation_summary(name: str) -> AgentEvaluationSummary:\n",
    "    \"\"\"\n",
    "    Loads an evaluation summary.\n",
    "    :param name: name\n",
    "    :return: loaded evaluation summary\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{name}.json'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return AgentEvaluationSummary.from_dict(json.load(f))\n"
   ],
   "id": "4ba3b6ea0235cd5e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train PPO Model\n",
    "The default PPO model is trained with a vectorized environment (number of environments: `8`).\n",
    "The number of total time steps is set to $2^{18}$ ($262'144$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d786b44a8df86031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:20:24.630577Z",
     "start_time": "2024-05-09T12:20:24.621480Z"
    }
   },
   "cell_type": "code",
   "source": "model_ppo_name = 'ppo_r4_64x64_c16_t2xx18_lrfix'",
   "id": "434a25c3f7eb010",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "if ENABLE_TRAINING:\n",
    "    vec_env = make_vec_env(lambda: experiment.create_filtered_train_env(), n_envs=8)\n",
    "    model_ppo = PPO('MultiInputPolicy', vec_env, verbose=1, tensorboard_log='./artifacts/tensorboard')\n",
    "    model_ppo.learn(total_timesteps=2 ** 18, tb_log_name=model_ppo_name)\n",
    "    mean_reward, std_reward = evaluate_policy(model_ppo, model_ppo.get_env(), n_eval_episodes=EVAL_N_EPISODES)\n",
    "    print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "    model_ppo.save(f'./artifacts/models/{model_ppo_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:49.416120Z",
     "start_time": "2024-05-09T12:20:24.630577Z"
    }
   },
   "id": "9ec883333cb3abb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./artifacts/tensorboard\\ppo_r4_64x64_c16_t2xx18_lrfix_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.37     |\n",
      "|    ep_rew_mean     | -0.599   |\n",
      "| time/              |          |\n",
      "|    fps             | 1057     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.72       |\n",
      "|    ep_rew_mean          | -0.365     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02862142 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.58      |\n",
      "|    explained_variance   | -0.162     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0288     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 0.209      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 4.69      |\n",
      "|    ep_rew_mean          | -0.239    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 556       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 88        |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0229549 |\n",
      "|    clip_fraction        | 0.536     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.52     |\n",
      "|    explained_variance   | 0.115     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0181   |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0738   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.69       |\n",
      "|    ep_rew_mean          | -0.076     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 515        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 127        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04728269 |\n",
      "|    clip_fraction        | 0.544      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.159      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0188     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0699    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.83       |\n",
      "|    ep_rew_mean          | 0.259      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 496        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 164        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02202673 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0231     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0498    |\n",
      "|    value_loss           | 0.171      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.87        |\n",
      "|    ep_rew_mean          | 0.469       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023335595 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0316      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.52        |\n",
      "|    ep_rew_mean          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026879104 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.06       |\n",
      "|    ep_rew_mean          | 0.888      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 281        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04522179 |\n",
      "|    clip_fraction        | 0.388      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.828     |\n",
      "|    explained_variance   | 0.344      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0237     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0626    |\n",
      "|    value_loss           | 0.205      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.17        |\n",
      "|    ep_rew_mean          | 1.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059783272 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.01       |\n",
      "|    ep_rew_mean          | 1.25       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 456        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 358        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05917614 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.399     |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00583   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    value_loss           | 0.0502     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 6.07         |\n",
      "|    ep_rew_mean          | 1.3          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 397          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052329944 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.305        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00663     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    value_loss           | 0.0122       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.35        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024220187 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.00518     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 445         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010312455 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.78        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 441         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059031665 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.04        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 439         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021356557 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    value_loss           | 0.0133      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.1        |\n",
      "|    ep_rew_mean          | 1.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 437        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 599        |\n",
      "|    total_timesteps      | 262144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06296337 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.318     |\n",
      "|    explained_variance   | 0.576      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0543    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    value_loss           | 0.00424    |\n",
      "----------------------------------------\n",
      "Mean reward: 1.3375000000000004 +/- 4.440892098500626e-16\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate PPO Model\n",
    "The trained PPO model is evaluated on $100$ time steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c56f2a1770a1d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Episodes for PPO",
   "id": "3bb06192170b24e"
  },
  {
   "cell_type": "code",
   "source": [
    "unwrapped_env, filtered_env = experiment.created_eval_env_tuple()\n",
    "model_ppo = PPO.load(f'./artifacts/models/{model_ppo_name}')\n",
    "obs, info = filtered_env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model_ppo.predict(obs)\n",
    "    decoded_action = filtered_env.decode_action(action)\n",
    "    obs, rewards, terminated, truncated, info = filtered_env.step(action)\n",
    "    print(\n",
    "        f'action: {decoded_action.type.name}-{decoded_action.operation.name if decoded_action.operation is not None else None} = {float(rewards)}')\n",
    "    if terminated or truncated:\n",
    "        obs, info = filtered_env.reset()\n",
    "        print(f'Episode end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:49.955884Z",
     "start_time": "2024-05-09T12:30:49.416120Z"
    }
   },
   "id": "37aa87ac5cc986",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-sum = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-merge = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-merge = 0.0625\n",
      "action: AppendOperation-sum = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-sum = 0.05\n",
      "action: AppendOperation-merge = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-sum = 0.05\n",
      "action: AppendOperation-merge = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-merge = 0.0625\n",
      "action: AppendOperation-sum = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: AppendOperation-sum = 0.0125\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n",
      "action: AppendOperation-sum = 0.075\n",
      "action: AppendOperation-sum = 0.0625\n",
      "action: AppendOperation-merge = 0.05\n",
      "action: AppendOperation-sum = 0.0375\n",
      "action: AppendOperation-sum = 0.025\n",
      "action: Stop-None = 1.0\n",
      "Episode end\n",
      "action: AppendOperation-split = 0.0875\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Agent Evaluation for PPO",
   "id": "1561835449c805e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:52.967431Z",
     "start_time": "2024-05-09T12:30:49.955884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.experiment.evaluate_agent import evaluate_agent\n",
    "\n",
    "if ENABLE_EVALUATION:\n",
    "    evaluation_ppo = evaluate_agent(\n",
    "            experiment,\n",
    "            model_ppo_name,\n",
    "            EVAL_N_EPISODES,\n",
    "            lambda obs: model_ppo.predict(obs)[0]\n",
    "    )\n",
    "    store_evaluation_summary(evaluation_ppo.summary)\n",
    "    \n",
    "evaluation_ppo_summary = load_evaluation_summary(model_ppo_name)\n",
    "evaluation_ppo_summary.solved_rate_train_complexities, evaluation_ppo_summary.solved_rate_eval_complexities"
   ],
   "id": "2e082c5b5436d544",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T12:30:52.974544Z",
     "start_time": "2024-05-09T12:30:52.967431Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation_ppo_summary.avg_n_operations_per_complexity",
   "id": "e8f5446e014fb36a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 8.06}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent is able to solve the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ffa0a5040885f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
