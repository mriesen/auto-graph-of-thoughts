{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Simple PPO Approach\n",
    "As a first approach with Deep Reinforcement Learning (DRL), a simple PPO agent is trained on lists of fixed cardinality.\n",
    "The goal of this first DRL approach is to verify that a complex Reinforcement Learning agent is able to learn a task for a given cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8769a32849ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of cardinality $16$.\n",
    "The complexity equals the list cardinality.\n"
   ],
   "id": "c3227651ebe35466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:46:44.263940Z",
     "start_time": "2024-05-26T09:46:43.157435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env import GraphObservationComponent, GraphStepRewardVersion\n",
    "from auto_graph_of_thoughts.experiment import ExperimentConfiguration, LanguageModelSimulationType, Experiment\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "ENABLE_TRAINING = True\n",
    "ENABLE_EVALUATION = True\n",
    "\n",
    "N_VEC_ENVS = 8\n",
    "\n",
    "COMPLEXITIES = [16]\n",
    "\n",
    "EVAL_N_EPISODES = 100\n",
    "\n",
    "config = ExperimentConfiguration(\n",
    "        seed=SEED,\n",
    "        task=sum_list_task,\n",
    "        max_steps=20,\n",
    "        observation_filter={\n",
    "            GraphObservationComponent.DEPTH,\n",
    "            GraphObservationComponent.BREADTH,\n",
    "            GraphObservationComponent.COMPLEXITY,\n",
    "            GraphObservationComponent.PREV_ACTIONS,\n",
    "            GraphObservationComponent.GRAPH_OPERATIONS,\n",
    "            GraphObservationComponent.LOCAL_COMPLEXITY,\n",
    "            GraphObservationComponent.PREV_SCORE\n",
    "        },\n",
    "        max_depth=8,\n",
    "        max_breadth=8,\n",
    "        divergence_cutoff_factor=0.5,\n",
    "        train_complexities=COMPLEXITIES,\n",
    "        eval_complexities=COMPLEXITIES,\n",
    "        max_complexity=max(COMPLEXITIES),\n",
    "        max_operations=32,\n",
    "        lm_simulation_type=LanguageModelSimulationType.REALISTIC,\n",
    "        reward_version=GraphStepRewardVersion.V5\n",
    ")\n",
    "experiment = Experiment(config)"
   ],
   "id": "59f8c0dfc11c7622",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "set_random_seed(SEED, True)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:46:47.722728Z",
     "start_time": "2024-05-26T09:46:44.265948Z"
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities",
   "id": "d842a88d911d4546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:46:47.730889Z",
     "start_time": "2024-05-26T09:46:47.723741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pure_graph_of_thoughts.api.schema import JsonSchemaEncoder\n",
    "import json\n",
    "import os\n",
    "from auto_graph_of_thoughts.experiment.agent_evaluation_summary import AgentEvaluationSummary\n",
    "\n",
    "results_directory = './artifacts/results/agent_evaluations'\n",
    "\n",
    "def store_evaluation_summary(evaluation_summary: AgentEvaluationSummary) -> None:\n",
    "    \"\"\"\n",
    "    Stores an evaluation summary to file.\n",
    "    :param evaluation_summary: evaluation summary to store\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{evaluation_summary.name}.json'\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_summary, f, cls=JsonSchemaEncoder, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_evaluation_summary(name: str) -> AgentEvaluationSummary:\n",
    "    \"\"\"\n",
    "    Loads an evaluation summary.\n",
    "    :param name: name\n",
    "    :return: loaded evaluation summary\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{name}.json'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return AgentEvaluationSummary.from_dict(json.load(f))\n"
   ],
   "id": "4ba3b6ea0235cd5e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train PPO Model\n",
    "The default PPO model is trained with a vectorized environment (number of environments: `8`).\n",
    "The number of total time steps is set to $2^{18}$ ($262'144$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d786b44a8df86031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:46:47.741111Z",
     "start_time": "2024-05-26T09:46:47.732897Z"
    }
   },
   "cell_type": "code",
   "source": "model_ppo_name = 'ppo_r5_64x64_c16_t2xx18_lrfix'",
   "id": "434a25c3f7eb010",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "if ENABLE_TRAINING:\n",
    "    vec_env = make_vec_env(lambda: experiment.create_filtered_train_env(), n_envs=N_VEC_ENVS, seed=SEED)\n",
    "    model_ppo = PPO('MultiInputPolicy', vec_env, seed=SEED, verbose=1, tensorboard_log='./artifacts/tensorboard')\n",
    "    model_ppo.learn(total_timesteps=2 ** 18, tb_log_name=model_ppo_name)\n",
    "    mean_reward, std_reward = evaluate_policy(model_ppo, model_ppo.get_env(), n_eval_episodes=EVAL_N_EPISODES)\n",
    "    print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "    model_ppo.save(f'./artifacts/models/{model_ppo_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:59:28.876341Z",
     "start_time": "2024-05-26T09:46:47.743626Z"
    }
   },
   "id": "9ec883333cb3abb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./artifacts/tensorboard\\ppo_r5_64x64_c16_t2xx18_lrfix_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.37     |\n",
      "|    ep_rew_mean     | -0.194   |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.69        |\n",
      "|    ep_rew_mean          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 405         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021467399 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.476      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.23        |\n",
      "|    ep_rew_mean          | -0.0553     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022028737 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.44        |\n",
      "|    ep_rew_mean          | 0.027       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022039156 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0166      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.99        |\n",
      "|    ep_rew_mean          | 0.294       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 364         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026520059 |\n",
      "|    clip_fraction        | 0.487       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00732     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.99       |\n",
      "|    ep_rew_mean          | 0.415      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 267        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02361989 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0586    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.99        |\n",
      "|    ep_rew_mean          | 0.703       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027969329 |\n",
      "|    clip_fraction        | 0.439       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | 0.95        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053279478 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.866      |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.44      |\n",
      "|    ep_rew_mean          | 1.23      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 370       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 398       |\n",
      "|    total_timesteps      | 147456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0608894 |\n",
      "|    clip_fraction        | 0.319     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.63     |\n",
      "|    explained_variance   | 0.426     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0305   |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.0557   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.14       |\n",
      "|    ep_rew_mean          | 1.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 442        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07344706 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.43      |\n",
      "|    explained_variance   | 0.355      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0112    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    value_loss           | 0.0344     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.73       |\n",
      "|    ep_rew_mean          | 1.3        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 489        |\n",
      "|    total_timesteps      | 180224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04151973 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.417     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.187      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    value_loss           | 0.00969    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.62        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021599866 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.012       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.72        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 580         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010863294 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.00454     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.03       |\n",
      "|    ep_rew_mean          | 1.31       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 627        |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08670144 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.348     |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    value_loss           | 0.0028     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.98        |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 361         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 679         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018442556 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0997      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.18        |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 360         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 726         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047761023 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "Mean reward: 1.3375000000000004 +/- 4.440892098500626e-16\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate PPO Model\n",
    "The trained PPO model is evaluated on $100$ time steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c56f2a1770a1d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Episodes for PPO",
   "id": "3bb06192170b24e"
  },
  {
   "cell_type": "code",
   "source": [
    "unwrapped_env, filtered_env = experiment.created_eval_env_tuple()\n",
    "model_ppo = PPO.load(f'./artifacts/models/{model_ppo_name}')\n",
    "obs, info = filtered_env.reset(seed=SEED)\n",
    "for i in range(100):\n",
    "    action, _states = model_ppo.predict(obs)\n",
    "    decoded_action = filtered_env.decode_action(action)\n",
    "    obs, rewards, terminated, truncated, info = filtered_env.step(action)\n",
    "    print(\n",
    "        f'action: {decoded_action.type.name}-{decoded_action.operation.name if decoded_action.operation is not None else None} = {float(rewards)}')\n",
    "    if terminated or truncated:\n",
    "        obs, info = filtered_env.reset()\n",
    "        print(f'Episode end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:59:29.646261Z",
     "start_time": "2024-05-26T09:59:28.879365Z"
    }
   },
   "id": "37aa87ac5cc986",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: APPEND_OPERATION-sum = 0.0\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-merge = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-sum = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Agent Evaluation for PPO",
   "id": "1561835449c805e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:59:34.230346Z",
     "start_time": "2024-05-26T09:59:29.648282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.experiment.evaluate_agent import evaluate_agent\n",
    "\n",
    "if ENABLE_EVALUATION:\n",
    "    evaluation_ppo = evaluate_agent(\n",
    "            experiment,\n",
    "            model_ppo_name,\n",
    "            EVAL_N_EPISODES,\n",
    "            lambda obs: model_ppo.predict(obs)[0]\n",
    "    )\n",
    "    store_evaluation_summary(evaluation_ppo.summary)\n",
    "    \n",
    "evaluation_ppo_summary = load_evaluation_summary(model_ppo_name)\n",
    "evaluation_ppo_summary.solved_rate_train_complexities, evaluation_ppo_summary.solved_rate_eval_complexities"
   ],
   "id": "2e082c5b5436d544",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:59:34.239989Z",
     "start_time": "2024-05-26T09:59:34.232354Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation_ppo_summary.avg_n_operations_per_complexity",
   "id": "e8f5446e014fb36a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 8.68}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent is able to solve the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ffa0a5040885f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
