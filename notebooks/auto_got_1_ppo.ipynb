{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Simple PPO Approach\n",
    "As a first approach with Deep Reinforcement Learning (DRL), a simple PPO agent is trained on lists of fixed cardinality.\n",
    "The goal of this first DRL approach is to verify that a complex Reinforcement Learning agent is able to learn a task for a given cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8769a32849ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of cardinality $16$.\n",
    "The complexity equals the list cardinality.\n"
   ],
   "id": "c3227651ebe35466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:55:40.064866Z",
     "start_time": "2024-05-16T14:55:38.892554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env import GraphObservationComponent, GraphStepRewardVersion\n",
    "from auto_graph_of_thoughts.experiment import ExperimentConfiguration, LanguageModelSimulationType, Experiment\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "ENABLE_TRAINING = True\n",
    "ENABLE_EVALUATION = True\n",
    "\n",
    "COMPLEXITIES = [16]\n",
    "\n",
    "EVAL_N_EPISODES = 100\n",
    "\n",
    "config = ExperimentConfiguration(\n",
    "        seed=SEED,\n",
    "        task=sum_list_task,\n",
    "        max_steps=20,\n",
    "        observation_filter={\n",
    "            GraphObservationComponent.DEPTH,\n",
    "            GraphObservationComponent.BREADTH,\n",
    "            GraphObservationComponent.COMPLEXITY,\n",
    "            GraphObservationComponent.PREV_ACTIONS,\n",
    "            GraphObservationComponent.GRAPH_OPERATIONS,\n",
    "            GraphObservationComponent.LOCAL_COMPLEXITY,\n",
    "            GraphObservationComponent.PREV_SCORE\n",
    "        },\n",
    "        max_depth=8,\n",
    "        max_breadth=8,\n",
    "        divergence_cutoff_factor=0.5,\n",
    "        train_complexities=COMPLEXITIES,\n",
    "        eval_complexities=COMPLEXITIES,\n",
    "        max_complexity=max(COMPLEXITIES),\n",
    "        max_operations=32,\n",
    "        lm_simulation_type=LanguageModelSimulationType.REALISTIC,\n",
    "        reward_version=GraphStepRewardVersion.V5\n",
    ")\n",
    "experiment = Experiment(config)"
   ],
   "id": "59f8c0dfc11c7622",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Reproducibility\n",
    "The seed for the PRNG is set to $0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7c93d09f7f257b"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "seed = 0\n",
    "set_random_seed(seed)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-16T14:55:43.272607Z",
     "start_time": "2024-05-16T14:55:40.066379Z"
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities",
   "id": "d842a88d911d4546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:55:43.279781Z",
     "start_time": "2024-05-16T14:55:43.273615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pure_graph_of_thoughts.api.schema import JsonSchemaEncoder\n",
    "import json\n",
    "import os\n",
    "from auto_graph_of_thoughts.experiment.agent_evaluation_summary import AgentEvaluationSummary\n",
    "\n",
    "results_directory = './artifacts/results/agent_evaluations'\n",
    "\n",
    "def store_evaluation_summary(evaluation_summary: AgentEvaluationSummary) -> None:\n",
    "    \"\"\"\n",
    "    Stores an evaluation summary to file.\n",
    "    :param evaluation_summary: evaluation summary to store\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{evaluation_summary.name}.json'\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_summary, f, cls=JsonSchemaEncoder, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_evaluation_summary(name: str) -> AgentEvaluationSummary:\n",
    "    \"\"\"\n",
    "    Loads an evaluation summary.\n",
    "    :param name: name\n",
    "    :return: loaded evaluation summary\n",
    "    \"\"\"\n",
    "    file_name = f'{results_directory}/{name}.json'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return AgentEvaluationSummary.from_dict(json.load(f))\n"
   ],
   "id": "4ba3b6ea0235cd5e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train PPO Model\n",
    "The default PPO model is trained with a vectorized environment (number of environments: `8`).\n",
    "The number of total time steps is set to $2^{18}$ ($262'144$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d786b44a8df86031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:55:43.287781Z",
     "start_time": "2024-05-16T14:55:43.281732Z"
    }
   },
   "cell_type": "code",
   "source": "model_ppo_name = 'ppo_r5_64x64_c16_t2xx18_lrfix'",
   "id": "434a25c3f7eb010",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "if ENABLE_TRAINING:\n",
    "    vec_env = make_vec_env(lambda: experiment.create_filtered_train_env(), n_envs=8)\n",
    "    model_ppo = PPO('MultiInputPolicy', vec_env, verbose=1, tensorboard_log='./artifacts/tensorboard')\n",
    "    model_ppo.learn(total_timesteps=2 ** 18, tb_log_name=model_ppo_name)\n",
    "    mean_reward, std_reward = evaluate_policy(model_ppo, model_ppo.get_env(), n_eval_episodes=EVAL_N_EPISODES)\n",
    "    print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "    model_ppo.save(f'./artifacts/models/{model_ppo_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T15:07:48.431170Z",
     "start_time": "2024-05-16T14:55:43.288789Z"
    }
   },
   "id": "9ec883333cb3abb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./artifacts/tensorboard\\ppo_r5_64x64_c16_t2xx18_lrfix_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.37     |\n",
      "|    ep_rew_mean     | -0.182   |\n",
      "| time/              |          |\n",
      "|    fps             | 801      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.69        |\n",
      "|    ep_rew_mean          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 501         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021617783 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.496      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.7         |\n",
      "|    ep_rew_mean          | -0.0431     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023879796 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00971     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.36        |\n",
      "|    ep_rew_mean          | 0.079       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 418         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024145762 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0214      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.06        |\n",
      "|    ep_rew_mean          | 0.246       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032339197 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.41        |\n",
      "|    ep_rew_mean          | 0.584       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023075197 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6          |\n",
      "|    ep_rew_mean          | 0.769      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 297        |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03045762 |\n",
      "|    clip_fraction        | 0.418      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0634    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.41        |\n",
      "|    ep_rew_mean          | 1.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037755754 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.844      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.22        |\n",
      "|    ep_rew_mean          | 1.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051613472 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.08       |\n",
      "|    ep_rew_mean          | 1.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 429        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09861677 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.452     |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.0347     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.53        |\n",
      "|    ep_rew_mean          | 1.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025099251 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.0108      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.55        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019560328 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000338    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.69        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022445077 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.00694     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.93        |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 613         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027033981 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.04        |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 656         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025535684 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00722    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.0352      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.19        |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021522082 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.00635     |\n",
      "-----------------------------------------\n",
      "Mean reward: 1.3375000000000004 +/- 4.440892098500626e-16\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate PPO Model\n",
    "The trained PPO model is evaluated on $100$ time steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c56f2a1770a1d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Episodes for PPO",
   "id": "3bb06192170b24e"
  },
  {
   "cell_type": "code",
   "source": [
    "unwrapped_env, filtered_env = experiment.created_eval_env_tuple()\n",
    "model_ppo = PPO.load(f'./artifacts/models/{model_ppo_name}')\n",
    "obs, info = filtered_env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model_ppo.predict(obs)\n",
    "    decoded_action = filtered_env.decode_action(action)\n",
    "    obs, rewards, terminated, truncated, info = filtered_env.step(action)\n",
    "    print(\n",
    "        f'action: {decoded_action.type.name}-{decoded_action.operation.name if decoded_action.operation is not None else None} = {float(rewards)}')\n",
    "    if terminated or truncated:\n",
    "        obs, info = filtered_env.reset()\n",
    "        print(f'Episode end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T15:07:49.049217Z",
     "start_time": "2024-05-16T15:07:48.432685Z"
    }
   },
   "id": "37aa87ac5cc986",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-merge = 0.0625\n",
      "action: APPEND_OPERATION-split = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-merge = 0.0625\n",
      "action: APPEND_OPERATION-split = 0.05\n",
      "action: APPEND_OPERATION-merge = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: APPEND_OPERATION-sum = 0.0125\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n",
      "action: APPEND_OPERATION-sum = 0.075\n",
      "action: APPEND_OPERATION-sum = 0.0625\n",
      "action: APPEND_OPERATION-merge = 0.05\n",
      "action: APPEND_OPERATION-sum = 0.0375\n",
      "action: APPEND_OPERATION-sum = 0.025\n",
      "action: STOP-None = 1.0\n",
      "Episode end\n",
      "action: APPEND_OPERATION-split = 0.0875\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Agent Evaluation for PPO",
   "id": "1561835449c805e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:07:52.506204Z",
     "start_time": "2024-05-16T15:07:49.050226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.experiment.evaluate_agent import evaluate_agent\n",
    "\n",
    "if ENABLE_EVALUATION:\n",
    "    evaluation_ppo = evaluate_agent(\n",
    "            experiment,\n",
    "            model_ppo_name,\n",
    "            EVAL_N_EPISODES,\n",
    "            lambda obs: model_ppo.predict(obs)[0]\n",
    "    )\n",
    "    store_evaluation_summary(evaluation_ppo.summary)\n",
    "    \n",
    "evaluation_ppo_summary = load_evaluation_summary(model_ppo_name)\n",
    "evaluation_ppo_summary.solved_rate_train_complexities, evaluation_ppo_summary.solved_rate_eval_complexities"
   ],
   "id": "2e082c5b5436d544",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:07:52.512715Z",
     "start_time": "2024-05-16T15:07:52.507221Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation_ppo_summary.avg_n_operations_per_complexity",
   "id": "e8f5446e014fb36a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 7.89}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent is able to solve the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ffa0a5040885f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
