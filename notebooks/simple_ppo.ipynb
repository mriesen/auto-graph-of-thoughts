{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Simple PPO Approach\n",
    "As a first approach with Deep Reinforcement Learning (DRL), a simple PPO agent is trained on lists of fixed cardinality.\n",
    "The goal of this first DRL approach is to verify that a complex Reinforcement Learning agent is able to learn a task for a given cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc8769a32849ffe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Reproducibility\n",
    "The seed for the PRNG is set to $0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7c93d09f7f257b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "seed = 0\n",
    "set_random_seed(seed)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:10:33.660208Z",
     "start_time": "2024-04-18T16:10:31.055884Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of cardinality $16$.\n",
    "The complexity equals the list cardinality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f560cb74b41545"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from auto_graph_of_thoughts.language_model import create_simulated_chat_gpt_sum_list\n",
    "from auto_graph_of_thoughts.controller import ContinuousGraphController\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "from pure_graph_of_thoughts.api.state import State\n",
    "\n",
    "MAX_DEPTH = 8\n",
    "MAX_BREADTH = 4\n",
    "DIVERGENCE_CUTOFF_FACTOR = 0.5\n",
    "\n",
    "CARDINALITIES = [16]\n",
    "COMPLEXITIES = CARDINALITIES\n",
    "MAX_COMPLEXITY = max(CARDINALITIES)\n",
    "\n",
    "_random = random.Random(seed)\n",
    "\n",
    "\n",
    "def generate_init_state() -> Tuple[int, State]:\n",
    "    complexity = _random.choice(COMPLEXITIES)\n",
    "    list_cardinality = complexity\n",
    "    init_state: State = {\n",
    "        'list': [\n",
    "            _random.randint(0, 9) for _ in range(list_cardinality)\n",
    "        ]\n",
    "\n",
    "    }\n",
    "    return complexity, init_state\n",
    "\n",
    "\n",
    "def create_controller() -> ContinuousGraphController:\n",
    "    return ContinuousGraphController(\n",
    "            language_model=create_simulated_chat_gpt_sum_list(seed),\n",
    "            generate_init_state=generate_init_state,\n",
    "            max_depth=MAX_DEPTH,\n",
    "            max_breadth=MAX_BREADTH,\n",
    "            divergence_cutoff_factor=DIVERGENCE_CUTOFF_FACTOR,\n",
    "            max_complexity=MAX_COMPLEXITY\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:10:34.292402Z",
     "start_time": "2024-04-18T16:10:33.661214Z"
    }
   },
   "id": "48eb1485642c8b9a",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate Environment\n",
    "The `GraphOfThoughtsEnv` environment is instantiated.\n",
    "The maximum time steps is set to $100$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7404469b99a7f3c5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\.conda\\envs\\auto-graph-of-thoughts\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:246: UserWarning: \u001B[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'auto_graph_of_thoughts.env.graph_step_reward.GraphStepReward'>\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Manuel\\.conda\\envs\\auto-graph-of-thoughts\\Lib\\site-packages\\gymnasium\\utils\\env_checker.py:321: UserWarning: \u001B[33mWARN: Not able to test alternative render modes due to the environment not having a spec. Try instantialising the environment through gymnasium.make\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from auto_graph_of_thoughts.env import GraphOfThoughtsEnv\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "\n",
    "def create_env() -> GraphOfThoughtsEnv:\n",
    "    return GraphOfThoughtsEnv(\n",
    "            sum_list_task,\n",
    "            create_controller(),\n",
    "            seed=seed,\n",
    "            max_steps=100\n",
    "    )\n",
    "\n",
    "\n",
    "env = create_env()\n",
    "check_env(env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:10:34.310521Z",
     "start_time": "2024-04-18T16:10:34.293428Z"
    }
   },
   "id": "6bcf31c418a7a609",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train PPO Model\n",
    "The default PPO model is trained with a vectorized environment (number of environments: `8`).\n",
    "The number of total time steps is set to $2^{18}$ ($262'144$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d786b44a8df86031"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.34     |\n",
      "|    ep_rew_mean     | -0.581   |\n",
      "| time/              |          |\n",
      "|    fps             | 1231     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.03       |\n",
      "|    ep_rew_mean          | -0.351     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 701        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03127069 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.58      |\n",
      "|    explained_variance   | -0.173     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00892    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.99        |\n",
      "|    ep_rew_mean          | -0.213      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022487206 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.3        |\n",
      "|    ep_rew_mean          | -0.0479    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 565        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05102094 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0684    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.57        |\n",
      "|    ep_rew_mean          | 0.186       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022174995 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.49        |\n",
      "|    ep_rew_mean          | 0.258       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 507         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016805623 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.97        |\n",
      "|    ep_rew_mean          | 0.371       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025257485 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.05       |\n",
      "|    ep_rew_mean          | 0.752      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 272        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03261091 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.918     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0844     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 0.235      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.15        |\n",
      "|    ep_rew_mean          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049639136 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074245155 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    value_loss           | 0.0989      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.25        |\n",
      "|    ep_rew_mean          | 1.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040055573 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.037       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015112873 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.035       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.43        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023800485 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00196    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 451         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 507         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023488093 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00105    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.0108      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.85       |\n",
      "|    ep_rew_mean          | 1.29       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 448        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 548        |\n",
      "|    total_timesteps      | 245760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03140126 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.3       |\n",
      "|    explained_variance   | 0.532      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    value_loss           | 0.00678    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.16       |\n",
      "|    ep_rew_mean          | 1.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 444        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 589        |\n",
      "|    total_timesteps      | 262144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06773947 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.252     |\n",
      "|    explained_variance   | 0.75       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0772    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 0.00242    |\n",
      "----------------------------------------\n",
      "Mean reward: 1.3375 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env(lambda: create_env(), n_envs=8)\n",
    "model_ppo = PPO('MultiInputPolicy', vec_env, verbose=1)\n",
    "model_ppo.learn(total_timesteps=2 ** 18)\n",
    "mean_reward, std_reward = evaluate_policy(model_ppo, model_ppo.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:20:50.588327Z",
     "start_time": "2024-04-18T16:10:34.311531Z"
    }
   },
   "id": "9ec883333cb3abb1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_ppo.save('./models/simple_ppo')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:20:50.605855Z",
     "start_time": "2024-04-18T16:20:50.588327Z"
    }
   },
   "id": "b3674ffae295ecb7",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate PPO Model\n",
    "The trained PPO model is evaluated on $100$ time steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c56f2a1770a1d1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-generate_single = 0.05\n",
      "depth: 4 - action: AppendOperation-generate_single = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n",
      "depth: 6 - action: AppendOperation-generate_single = 0.0125\n",
      "depth: 7 - action: AppendOperation-generate_single = 0.0\n",
      "depth: 8 - action: AppendOperation-generate_single = -0.1\n",
      "depth: 8 - action: AppendOperation-generate_single = -0.1\n",
      "depth: 8 - action: Stop-None = 1.0\n",
      "Episode end\n",
      "depth: 0 - action: AppendOperation-split = 0.0875\n",
      "depth: 1 - action: AppendOperation-generate_single = 0.075\n",
      "depth: 2 - action: AppendOperation-merge = 0.0625\n",
      "depth: 3 - action: AppendOperation-split = 0.05\n",
      "depth: 4 - action: AppendOperation-merge = 0.0375\n",
      "depth: 5 - action: AppendOperation-generate_single = 0.025\n"
     ]
    }
   ],
   "source": [
    "from auto_graph_of_thoughts.env import ObservationComponent\n",
    "\n",
    "env = create_env()\n",
    "model_ppo = PPO.load('./models/simple_ppo')\n",
    "obs, info = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model_ppo.predict(obs)\n",
    "    depth = obs[ObservationComponent.depth.value].item()\n",
    "    decoded_action = env.decode_action(action)\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    print(\n",
    "        f'depth: {depth} - action: {decoded_action.type.name}-{decoded_action.operation.name if decoded_action.operation is not None else None} = {float(rewards)}')\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        print(f'Episode end')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T16:21:51.250008Z",
     "start_time": "2024-04-18T16:21:50.788117Z"
    }
   },
   "id": "37aa87ac5cc986",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent is able to solve the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ffa0a5040885f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
