{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated Graph of Thoughts - Reward Shaping\n",
    "This notebook demonstrates the applied reward shaping process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da8e87edce92263d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Reproducibility\n",
    "The seed for the PRNG is set to $0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5bddc0d7d328f0c"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "SEED = 0\n",
    "set_random_seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:47.537953Z",
     "start_time": "2024-05-26T09:44:43.041451Z"
    }
   },
   "id": "b52ff94e6cc26024",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Provide Required Components with Parameters\n",
    "Factory function for the required components are provided.\n",
    "The experiment is employed with the following parameters:\n",
    "- maximum graph depth: $8$\n",
    "- maximum graph breadth: $4$\n",
    "- divergence cutoff factor: $0.5$\n",
    "\n",
    "The model is trained solely on lists of length $16$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21843e1a2b44be91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:48.438539Z",
     "start_time": "2024-05-26T09:44:47.540965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env import GraphObservationComponent, GraphStepRewardVersion\n",
    "from auto_graph_of_thoughts.experiment import ExperimentConfiguration, LanguageModelSimulationType, Experiment\n",
    "from auto_graph_of_thoughts.tasks.sum_list import sum_list_task\n",
    "\n",
    "COMPLEXITIES = [16]\n",
    "\n",
    "base_config = ExperimentConfiguration(\n",
    "        seed=SEED,\n",
    "        task=sum_list_task,\n",
    "        max_steps=20,\n",
    "        observation_filter={\n",
    "            GraphObservationComponent.DEPTH\n",
    "        },\n",
    "        max_depth=8,\n",
    "        max_breadth=4,\n",
    "        divergence_cutoff_factor=0.5,\n",
    "        train_complexities=COMPLEXITIES,\n",
    "        eval_complexities=COMPLEXITIES,\n",
    "        max_complexity=max(COMPLEXITIES),\n",
    "        max_operations=32,\n",
    "        lm_simulation_type=LanguageModelSimulationType.DETERMINISTIC,\n",
    "        reward_version=GraphStepRewardVersion.V0\n",
    ")\n",
    "base_experiment = Experiment(base_config)"
   ],
   "id": "d23c7bd8e5843738",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\n",
    "To reduce the observation space to a discrete shape, the actual environment is wrapped in a discrete observation projection wrapper.\n",
    "The resulting `depth_env` is used for the training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61eccda5083deb83"
  },
  {
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.env.wrapper import OrdinalDiscreteToDiscreteObsMappingWrapper, \\\n",
    "    OrdinalDiscreteObsFilterWrapper\n",
    "from auto_graph_of_thoughts.env import GraphOfThoughtsEnv, GraphObservationComponent\n",
    "\n",
    "def create_discrete_depth_env(graph_of_thoughts_env: GraphOfThoughtsEnv):\n",
    "    component = GraphObservationComponent.DEPTH\n",
    "    return OrdinalDiscreteToDiscreteObsMappingWrapper(\n",
    "            OrdinalDiscreteObsFilterWrapper(\n",
    "                    graph_of_thoughts_env, component\n",
    "            ),\n",
    "            graph_of_thoughts_env.observation_space[component.value]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:48.444611Z",
     "start_time": "2024-05-26T09:44:48.439547Z"
    }
   },
   "id": "f48211167ac72e36",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up Training\n",
    "The Q-learning agent is trained on a total of $1000$ episodes with the following parameters:\n",
    "- learning rate $\\alpha$: $0.1$\n",
    "- discount factor $\\gamma$: $0.99$\n",
    "- exploration rate $\\epsilon$: $0.1$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25aebdd7399e35c"
  },
  {
   "cell_type": "code",
   "source": [
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:48.453430Z",
     "start_time": "2024-05-26T09:44:48.445621Z"
    }
   },
   "id": "96f2db20ebbbd317",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The reward function under test is evaluated as follows.\n",
    "\n",
    "The Q-Learning instance is trained iteratively on $100$ episodes over $10$ iterations.\n",
    "After each training, a single episode is evaluated.\n",
    "\n",
    "The results are printed for interpretation."
   ],
   "id": "1477a3a70ebbd65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:48.466770Z",
     "start_time": "2024-05-26T09:44:48.456956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from auto_graph_of_thoughts.rl import QLearning\n",
    "\n",
    "EPISODES_PER_ITERATION = 100\n",
    "N_ITERATIONS = 10\n",
    "\n",
    "def evaluate_reward(experiment: Experiment) -> None:\n",
    "    env = experiment.create_unwrapped_train_env()\n",
    "    depth_env = create_discrete_depth_env(env)\n",
    "    q_learning = QLearning(depth_env, ALPHA, GAMMA, EPSILON, seed=SEED)\n",
    "    for i in range(1, N_ITERATIONS+1):\n",
    "        q_learning.learn(total_episodes=EPISODES_PER_ITERATION)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_rewards = 0\n",
    "        state, _ = depth_env.reset()\n",
    "        while not terminated and not truncated:\n",
    "            action = q_learning.predict(state)\n",
    "            state, reward, terminated, truncated, _ = depth_env.step(action)\n",
    "            reward = float(reward)\n",
    "            total_rewards += reward\n",
    "        print(f'trained episodes: {i * EPISODES_PER_ITERATION}, total rewards: {total_rewards}, solved: {env.is_solved}, n operations: {env.n_operations}')"
   ],
   "id": "1e21e6f956ef529c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V1: Test Sparse Reward\n",
    "A simple reward function that only rewards the correct final result.\n",
    "A penalty of ```-(10 / self.max_depth) * self.depth``` is applied."
   ],
   "id": "7464be8debedef02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:48.744846Z",
     "start_time": "2024-05-26T09:44:48.468779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r1_config = replace(base_config, reward_version=GraphStepRewardVersion.V1)\n",
    "r1_experiment = Experiment(r1_config)\n",
    "evaluate_reward(r1_experiment)"
   ],
   "id": "2f898a066e78f3de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 200, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 300, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 400, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 500, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 600, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 700, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 800, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 900, total rewards: 0.0, solved: False, n operations: 0\n",
      "trained episodes: 1000, total rewards: 0.0, solved: False, n operations: 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are no solved tasks.",
   "id": "e8dc9aa51d14b2e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V2: Test Additional Invalid Signal\n",
    "The reward function is adjusted to penalize invalid actions."
   ],
   "id": "cb917f98174c9fa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:44:53.866017Z",
     "start_time": "2024-05-26T09:44:48.745855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r2_config = replace(base_config, reward_version=GraphStepRewardVersion.V2)\n",
    "r2_experiment = Experiment(r2_config)\n",
    "evaluate_reward(r2_experiment)"
   ],
   "id": "6aee7e9f989cb790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: -2.0125000000000006, solved: False, n operations: 1\n",
      "trained episodes: 200, total rewards: -2.0000000000000004, solved: False, n operations: 0\n",
      "trained episodes: 300, total rewards: -2.125000000000001, solved: False, n operations: 11\n",
      "trained episodes: 400, total rewards: -2.0750000000000006, solved: False, n operations: 3\n",
      "trained episodes: 500, total rewards: -2.2625000000000006, solved: False, n operations: 15\n",
      "trained episodes: 600, total rewards: -2.0000000000000004, solved: False, n operations: 0\n",
      "trained episodes: 700, total rewards: -0.625, solved: False, n operations: 11\n",
      "trained episodes: 800, total rewards: -3.225, solved: False, n operations: 18\n",
      "trained episodes: 900, total rewards: -1.1500000000000001, solved: False, n operations: 23\n",
      "trained episodes: 1000, total rewards: -0.3375, solved: False, n operations: 3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unfortunately, there are no solved tasks.",
   "id": "b8aa8c30fcde0267"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V3: Test Additional Intermediate Rewards\n",
    "Now, intermediate rewards are introduced.\n",
    "Scored actions are now rewarded, as well as non-scored valid ones, while scored actions with a negative score are penalized."
   ],
   "id": "3d1976892011eca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:45:04.872535Z",
     "start_time": "2024-05-26T09:44:53.867024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r3_config = replace(base_config, reward_version=GraphStepRewardVersion.V3)\n",
    "r3_experiment = Experiment(r3_config)\n",
    "evaluate_reward(r3_experiment)"
   ],
   "id": "4426223dda723661",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: 1.9375000000000007, solved: False, n operations: 23\n",
      "trained episodes: 200, total rewards: 1.9375000000000007, solved: False, n operations: 23\n",
      "trained episodes: 300, total rewards: 1.9375000000000007, solved: False, n operations: 23\n",
      "trained episodes: 400, total rewards: 1.9375000000000007, solved: False, n operations: 23\n",
      "trained episodes: 500, total rewards: 1.9375000000000007, solved: False, n operations: 23\n",
      "trained episodes: 600, total rewards: 1.9375000000000007, solved: False, n operations: 21\n",
      "trained episodes: 700, total rewards: 1.9375000000000007, solved: False, n operations: 21\n",
      "trained episodes: 800, total rewards: 1.9375000000000007, solved: False, n operations: 21\n",
      "trained episodes: 900, total rewards: 1.9375000000000007, solved: False, n operations: 21\n",
      "trained episodes: 1000, total rewards: 1.9375000000000007, solved: False, n operations: 21\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Still, there are no solved tasks.",
   "id": "436ee8fab4c9059f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V4: Test Additional Backtrack Penalty\n",
    "The backtrack action indicates that there must be another append action, which should be rated negatively."
   ],
   "id": "aa0be99d58a2fceb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:45:11.684860Z",
     "start_time": "2024-05-26T09:45:04.874053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r4_config = replace(base_config, reward_version=GraphStepRewardVersion.V4)\n",
    "r4_experiment = Experiment(r4_config)\n",
    "evaluate_reward(r4_experiment)"
   ],
   "id": "f49330fce894d8f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: 0.14999999999999997, solved: False, n operations: 13\n",
      "trained episodes: 200, total rewards: 0.14999999999999997, solved: False, n operations: 13\n",
      "trained episodes: 300, total rewards: 1.35, solved: True, n operations: 11\n",
      "trained episodes: 400, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 500, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 600, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 700, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 800, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 900, total rewards: 1.35, solved: True, n operations: 9\n",
      "trained episodes: 1000, total rewards: 1.35, solved: True, n operations: 9\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As one can see, the algorithm is able to solve the tasks after the first $300$ training episodes with $11$ operations.\n",
    "After $400$ episodes, there are only $9$ operations required."
   ],
   "id": "3be196c9d9b90a1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## V5: Test Complex Backtrack Penalty\n",
    "A backtrack action must be rated less negatively if it follows a negatively scored operation.\n",
    "Therefore, there is now a complex backtrack penalty in place."
   ],
   "id": "623293b1de696d5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:45:17.230264Z",
     "start_time": "2024-05-26T09:45:11.685866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r5_config = replace(base_config, reward_version=GraphStepRewardVersion.V5)\n",
    "r5_experiment = Experiment(r5_config)\n",
    "evaluate_reward(r5_experiment)"
   ],
   "id": "5d03bc0454a15e2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: 1.35, solved: True, n operations: 11\n",
      "trained episodes: 200, total rewards: -0.06250000000000006, solved: False, n operations: 13\n",
      "trained episodes: 300, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 400, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 500, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 600, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 700, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 800, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 900, total rewards: 1.3375, solved: True, n operations: 9\n",
      "trained episodes: 1000, total rewards: 1.3375, solved: True, n operations: 9\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After $300$ episodes, the task is solved with only $9$ operations.\n",
    "This is the best result so far."
   ],
   "id": "fa086aa9657aa46a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "27a5606056833528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V6: Test Operation Penalty\n",
    "Instead of using the depth as a penalty factor, the number of operations should be used.\n",
    "This reflects the actual cost better than the depth, which might lead to a better result."
   ],
   "id": "e8460223b79d9356"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:45:24.900204Z",
     "start_time": "2024-05-26T09:45:17.231275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "r6_config = replace(base_config, reward_version=GraphStepRewardVersion.V6)\n",
    "r6_experiment = Experiment(r6_config)\n",
    "evaluate_reward(r6_experiment)"
   ],
   "id": "784f115f951800dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained episodes: 100, total rewards: 0.18749999999999997, solved: False, n operations: 27\n",
      "trained episodes: 200, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 300, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 400, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 500, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 600, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 700, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 800, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 900, total rewards: 1.61875, solved: True, n operations: 12\n",
      "trained episodes: 1000, total rewards: 1.61875, solved: True, n operations: 12\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The task is solved after $200$ episodes of training.\n",
    "However, there are more operations required, the number of operations stays constantly at $12$."
   ],
   "id": "176a604bfe1b0080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reward Function Selection\n",
    "The reward function `V5` is selected as the general reward function to use.\n",
    "With reward function `V5`, the Q-Learning agent required the least number of episodes for the lowest number of operations to solve the task.\n",
    "For further investigation, `V4` and `V6` are kept as candidates."
   ],
   "id": "70b8f5e136c568f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
